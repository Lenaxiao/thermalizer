{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10dedae30>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_TOKEN = 0\n",
    "SOS_TOKEN = 1\n",
    "EOS_TOKEN = 2\n",
    "\n",
    "class BuildVocab:\n",
    "    def __init__(self):\n",
    "        self.word2index = {'<PAD>': PAD_TOKEN, '<SOS>': SOS_TOKEN, '<EOS>': EOS_TOKEN}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_TOKEN:'<PAD>', SOS_TOKEN: '<SOS>', EOS_TOKEN: '<EOS>'}\n",
    "        self.num_words = 3\n",
    "    \n",
    "    def fromCorpus(self, corpus):\n",
    "        for line in corpus:\n",
    "            self.fromSentence(line)\n",
    "    \n",
    "    def fromSentence(self, line):\n",
    "        for word in line:\n",
    "            self.toVocab(word)\n",
    "    \n",
    "    def toVocab(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trimming sentence from:  (999, 4)\n",
      "To:  (715, 4)\n",
      "Word Count:  24\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"protein_seq_small.csv\", index_col=0)\n",
    "df.drop_duplicates(inplace=True)\n",
    "print(\"Trimming sentence from: \", df.shape)\n",
    "\n",
    "# chop off the sequence length\n",
    "# keep sentence length <= 500\n",
    "mask = (df[\"meso_seq\"].str.len()<=500)&(df[\"thermal_seq\"].str.len()<=500)\n",
    "df = df.loc[mask].reset_index(drop=True)\n",
    "print(\"To: \", df.shape)\n",
    "\n",
    "input_seq = df[\"meso_seq\"].tolist()\n",
    "target_seq = df[\"thermal_seq\"].tolist()\n",
    "\n",
    "voc = BuildVocab()\n",
    "voc.fromCorpus(input_seq + target_seq)\n",
    "print(\"Word Count: \", len(voc.word2index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping character to index for all sentences\n",
    "def map_character(voc, line):\n",
    "    return [voc.word2index[word] for word in line] + [EOS_TOKEN] # EOS token\n",
    "\n",
    "def sequence_padding(ctx, max_len):\n",
    "    for index, seq in enumerate(ctx):\n",
    "        pad_to_seq = [PAD_TOKEN]*(max_len - len(seq))\n",
    "        ctx[index] = [*seq, *pad_to_seq]\n",
    "    return ctx\n",
    "\n",
    "def target_binary_mask(ctx):\n",
    "    for i, line in enumerate(ctx):\n",
    "        for j, index in enumerate(line):\n",
    "            if index != 0:\n",
    "                ctx[i][j] = 1\n",
    "    return ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input and output batch shape should be (max_length, batch_size)\n",
    "def input_init(input_batch, voc):\n",
    "    mapped_batch = [map_character(voc, line) for line in input_batch]\n",
    "    lengths = [len(seq_len) for seq_len in mapped_batch]\n",
    "    length_t = torch.tensor(lengths)\n",
    "    max_input_len = max(lengths)\n",
    "    padded_batch = sequence_padding(mapped_batch, max_input_len)\n",
    "    padded_batch_t = torch.LongTensor(padded_batch).t()\n",
    "    return padded_batch_t, length_t\n",
    "\n",
    "def target_init(target_batch, voc):\n",
    "    mapped_batch = [map_character(voc, line) for line in target_batch]\n",
    "    max_target_len = max(len(seq_len) for seq_len in mapped_batch)\n",
    "    padded_batch = sequence_padding(mapped_batch, max_target_len)\n",
    "    padded_batch_t = torch.LongTensor(padded_batch).t()\n",
    "    target_mask = target_binary_mask(padded_batch)\n",
    "    target_mask_t = torch.ByteTensor(target_mask).t()\n",
    "    return max_target_len, padded_batch_t, target_mask_t\n",
    "\n",
    "def batch_inp_out(input_seq, target_seq, batch_size):\n",
    "    idx = np.random.choice(len(input_seq), size=batch_size)\n",
    "    input_batch = [input_seq[i] for i in idx]\n",
    "    target_batch = [target_seq[i] for i in idx]\n",
    "    # sort lengths to pack_padded_sequence later\n",
    "    input_batch, target_batch = zip(*sorted(zip(input_batch, target_batch),\n",
    "                                            key=lambda x:len(x[0]),\n",
    "                                            reverse=True))\n",
    "    input_batch = list(input_batch)\n",
    "    target_batch = list(target_batch)\n",
    "    input_t, lengths = input_init(input_batch, voc)\n",
    "    max_target_len, target_t, mask_t = target_init(target_batch, voc)\n",
    "    return input_t, lengths, max_target_len, target_t, mask_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input sequences tensor: \n",
      " tensor([[ 3,  3,  3,  3,  3],\n",
      "        [20, 12,  4, 10,  5],\n",
      "        [12, 12, 16, 12, 14],\n",
      "        ...,\n",
      "        [ 7,  0,  0,  0,  0],\n",
      "        [13,  0,  0,  0,  0],\n",
      "        [ 2,  0,  0,  0,  0]])\n",
      "input length tensor: \n",
      " tensor([495, 441, 337, 321, 228])\n",
      "max target length:  275\n",
      "target sequences tensor: \n",
      " tensor([[ 3,  3,  3,  3,  3],\n",
      "        [21, 12, 12, 16, 12],\n",
      "        [13,  6, 16, 14, 14],\n",
      "        ...,\n",
      "        [ 0,  0,  0, 22,  0],\n",
      "        [ 0,  0,  0, 16,  0],\n",
      "        [ 0,  0,  0,  2,  0]])\n",
      "target binary mask: \n",
      " tensor([[1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1],\n",
      "        ...,\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 0],\n",
      "        [0, 0, 0, 1, 0]], dtype=torch.uint8)\n"
     ]
    }
   ],
   "source": [
    "### test\n",
    "batch_size = 5\n",
    "batches = batch_inp_out(input_seq, target_seq, batch_size)\n",
    "input_t, lengths, max_target_len, target_t, mask_t = batches\n",
    "print(\"input sequences tensor: \\n\", input_t)\n",
    "print(\"input length tensor: \\n\", lengths)\n",
    "print(\"max target length: \", max_target_len)\n",
    "print(\"target sequences tensor: \\n\", target_t)\n",
    "print(\"target binary mask: \\n\", mask_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Neural Network\n",
    "## Part I: Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network inherit from nn.Module (container) and override forward() method\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, hidden_dim, embedding, layer_dim, dropout=0):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layer_dim, self.hidden_dim, self.embedding = layer_dim, hidden_dim, embedding\n",
    "        # nn.GRU(input_dim, hidden_dim, layer_dim)\n",
    "        self.gru = nn.GRU(hidden_dim, hidden_dim, layer_dim, dropout=dropout, bidirectional=True)\n",
    "    \n",
    "    # input_seq: (max_length, b atch_size)\n",
    "    # input_length: (batch_size)\n",
    "    def forward(self, input_seq, input_length):\n",
    "        \n",
    "        # for each batch_size add a random feature with shape of embedding_dim\n",
    "        # embeddings: (max_length, batch_size, embedding_dim=hidden_dim)\n",
    "        embeddings = self.embedding(input_seq)\n",
    "        \n",
    "        # tensor will be concated in time axis\n",
    "        # basically, it will again concat the first column (seq), second, ....\n",
    "        # pack: (sum_seq_len, num_directions * hidden_size)\n",
    "        pack = torch.nn.utils.rnn.pack_padded_sequence(embeddings, input_length)\n",
    "        \n",
    "        # hidden: (n_layers * num_directions, batch_size, hidden_size)\n",
    "        outputs, hidden = self.gru(pack)\n",
    "        \n",
    "        # unpack outputs: (max_length, batch_size, num_direction * hidden_size)\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        \n",
    "        # sum bidirectional outputs\n",
    "        outputs = outputs[:, :, :self.hidden_dim] + outputs[:, :, self.hidden_dim:]\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape:  torch.Size([5, 3, 40])\n",
      "hidden shape: torch.Size([4, 3, 20])\n",
      "output shape:  torch.Size([5, 3, 20])\n"
     ]
    }
   ],
   "source": [
    "## dimension test\n",
    "rnn = nn.GRU(6, 20, 2, bidirectional=True)  # (input_size = features, hidden_size, n_layers)\n",
    "inputs = torch.randn(5, 3, 6)  # (max_length, batch_size, input_size)\n",
    "h0 = torch.randn(2*2, 3, 20)  # (n_layers * num_direction, batch_size, hidden_size)\n",
    "output, hn = rnn(inputs, h0)\n",
    "print(\"Output shape: \", output.shape)  # (max_length, batch_size, hidden_size)\n",
    "print(\"hidden shape:\", hn.shape)  # (n_layers * num_direction, batch_size, hidden_size)\n",
    "output = output[:, :, :20] + output[:, :, 20:]\n",
    "print(\"output shape: \", output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1,  6,  9,  2,  7, 10,  3,  8,  4,  5])\n",
      "tensor([3, 3, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "## something about pack_padded_sequence...\n",
    "# (batch_size, max_length)\n",
    "a = torch.tensor([[1, 2, 3, 4, 5], [6, 7, 8, 0, 0], [9, 10, 0, 0, 0]])\n",
    "length = [5, 3, 2]\n",
    "targets = nn.utils.rnn.pack_padded_sequence(a, length, batch_first=True)\n",
    "print(targets[0]) # concat first element of every batch, second element of every batch...\n",
    "print(targets[1]) # different batch size for each timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part II. Decoder: token by token until EOS\n",
    "Use current hidden state of the decoder and output of encoder.\n",
    "Global attention's score function:\n",
    "                      \n",
    "$$\n",
    "\\begin{equation}\n",
    "Scores = \n",
    "\\begin{cases}\n",
    "    h_t^T h_s & \\text{dot}\\\\\n",
    "    h_t W_a h_s & \\text{general}\\\\\n",
    "    v_a^T \\tanh(W_a[h_t;h_s]) & \\text{concat}\n",
    "\\end{cases}\n",
    "\\end{equation}\\\\\n",
    "$$\n",
    "Note: $h_t$ is the current target decoder state and $h_s$ is all encoder states (= encoder output). $W_a$ is the weight from linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    # current decoder state (ht): (1, batch_size, hidden_size) \n",
    "    # encoder output (hs): (max_length, batch_size, hidden_size)\n",
    "    # return: (max_length, batch_size)\n",
    "    def attn_dot(self, ht, hs):\n",
    "        return torch.sum(ht * hs, dim=2) # sum across hidden_size\n",
    "    \n",
    "    # return: (batch_size, 1, max_length)\n",
    "    def forward(self, ht, hs):\n",
    "        # (batch_size, max_length)\n",
    "        score = self.attn_dot(ht, hs).t()\n",
    "        # normalize across each row so that the sum of each col in a row will be 1\n",
    "        # and add one dimension\n",
    "        return F.softmax(score, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1, 10])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Test Attn\n",
    "att = Attn('dot', 20)\n",
    "ht = torch.randn(1, 5, 20)\n",
    "hs = torch.randn(10, 5, 20)\n",
    "att.forward(ht, hs).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    # note: input_size=feature size, output_size = vocabulary size\n",
    "    def __init__(self, method, embedding, hidden_size, output_size, n_layer=1, dropout=0):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.method, self.hidden_size, self.output_size, self.n_layer = method, hidden_size, output_size, n_layer\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        # embed_size = hidden_size here\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layer, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size) \n",
    "        self.attn = Attn(method, hidden_size)\n",
    "    \n",
    "    # decoder_input: (1, batch_size) one batch of words at a time\n",
    "    # decoder_hidden: (n_layer * num_directional, batch_size, hidden_size)\n",
    "    # encoder_output: (max_length, batch_size, hidden_size)\n",
    "    def forward(self, decoder_input, decoder_hidden, encoder_output):\n",
    "        # embeds: (1, batch_size, embed_size=hidden_size)\n",
    "        embeds = self.embedding_dropout(self.embedding(decoder_input))\n",
    "\n",
    "        # only one word, no need to pack padded sequence...\n",
    "        \n",
    "        # decoder_output: (1, batch_size, hidden_size)\n",
    "        # hidden: (n_layers * num_directions, batch_size, hidden_size)\n",
    "        decoder_output, hidden = self.gru(embeds, decoder_hidden)\n",
    "        # calculate attention weight\n",
    "        # score: (batch_size, 1, max_length)\n",
    "        score = self.attn(decoder_output, encoder_output)\n",
    "        # context: (batch_size, hidden_size)\n",
    "        context = score.bmm(encoder_output.transpose(0, 1)).squeeze(1)\n",
    "        # decoder_output: (batch_size, hidden_size)\n",
    "        decoder_output = decoder_output.squeeze(0)\n",
    "        # concat into (batch_size, hidden_size*2)\n",
    "        linear_input = torch.cat((decoder_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(linear_input))\n",
    "        # predict next word: (batch_size, vocab_size)\n",
    "        output = F.softmax(self.out(concat_output), dim=1)\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 5])\n",
      "tensor([[0.1435, 0.2657, 0.2132, 0.1228, 0.2548],\n",
      "        [0.1512, 0.2577, 0.1475, 0.1954, 0.2482],\n",
      "        [0.1516, 0.2442, 0.2489, 0.1068, 0.2485]], grad_fn=<SoftmaxBackward>)\n",
      "tensor([[0.2657],\n",
      "        [0.2577],\n",
      "        [0.2489]], grad_fn=<TopkBackward>)\n",
      "tensor([[1],\n",
      "        [1],\n",
      "        [2]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.2657, 0.2577, 0.2489], grad_fn=<MaxBackward0>), tensor([1, 1, 2]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test\n",
    "linear_input_t = torch.randn(3, 8)  # (batch_size, hidden_size)\n",
    "concat = nn.Linear(8, 4)\n",
    "concat_output_t = torch.tanh(concat(linear_input_t))\n",
    "out = nn.Linear(4, 5) \n",
    "output = F.softmax(out(concat_output_t), dim=1)\n",
    "print(output.shape)\n",
    "print(output)\n",
    "f, t = output.topk(1) # torch.max(decoder_output, dim=1)\n",
    "print(f)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate loss and ignore padded part\n",
    "# dec_out: (batch_size, vocab_size)\n",
    "def loss_func(dec_out, target, mask):\n",
    "    # (batch_size, 1)\n",
    "    target = target.view(-1, 1)\n",
    "    # negative log likelihood\n",
    "    crossEntropy = -torch.log(torch.gather(dec_out, 1, target).squeeze(1))\n",
    "    # select non-zero elements and calculate the mean\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, mask.sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.7583), 3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## test loss_func\n",
    "dec_out = torch.randn(5, 7)\n",
    "dec_out = F.softmax(dec_out, dim=1)\n",
    "target = torch.LongTensor([3, 5, 4, 1, 2])\n",
    "mask = torch.tensor([1, 1, 0, 0, 1], dtype=torch.uint8)\n",
    "loss_func(dec_out, target, mask)\n",
    "# print(\"Dec_out:\", dec_out)\n",
    "# target = target.view(-1, 1)\n",
    "# print(\"target:\", target)\n",
    "# gather = torch.gather(dec_out, 1, target).squeeze(1)\n",
    "# print(\"gather:\", gather)\n",
    "# crossEntropy = -torch.log(gather)\n",
    "# print(\"crossEntropy:\", crossEntropy)\n",
    "# loss = crossEntropy.masked_select(mask)\n",
    "# print(\"loss:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part III. Training Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train for a fixed iterations\n",
    "def train(input_seq, target_seq, batch_size, epochs, hidden_dim, embedding, encoder, decoder,\n",
    "          encoder_opt, decoder_opt, teacher_forcing_r, epoch_from, folder):\n",
    "    \n",
    "    for epoch in range(epoch_from, epochs):\n",
    "        n_iters = int(len(input_seq)/batch_size)\n",
    "        batches = [batch_inp_out(input_seq, target_seq, batch_size) for _ in range(n_iters)]\n",
    "        iter_loss = 0\n",
    "        for i in tqdm(range(n_iters)):\n",
    "            input_t, lengths, max_target_len, target_t, mask_t = batches[i]\n",
    "            # initialize and move the model to GPU/CPU\n",
    "            # set training mode\n",
    "            encoder_opt.zero_grad()\n",
    "            decoder_opt.zero_grad()\n",
    "\n",
    "            input_t = input_t.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            target_t = target_t.to(device)\n",
    "            mask_t = mask_t.to(device)\n",
    "\n",
    "            encoder_output, encoder_hidden = encoder(input_t, lengths)\n",
    "            decoder_input = torch.LongTensor([[SOS_TOKEN for _ in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # set initial decoder hidden state to the encoder's final state\n",
    "            decoder_hidden = encoder_hidden[:decoder.n_layer]\n",
    "\n",
    "            if i == 0:\n",
    "                print(\"Encoder output shape: \", encoder_output.shape)\n",
    "                print(\"Encoder hidden shape: \", encoder_hidden.shape)\n",
    "                print(\"Init decoder input: \", decoder_input.shape)\n",
    "                print(\"decoder hidden shape: \", decoder_hidden.shape)\n",
    "                print()\n",
    "\n",
    "            loss = 0\n",
    "            print_loss = []\n",
    "            nTotals = 0\n",
    "            if np.random.random() < teacher_forcing_r:\n",
    "                for step in range(max_target_len):\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "                    # teacher forcing: next input is current target\n",
    "                    decoder_input = target_t[step].view(1, -1)\n",
    "                    mask_loss, nTotal = loss_func(decoder_output, target_t[step], mask_t[step])\n",
    "                    loss += mask_loss\n",
    "                    print_loss.append(mask_loss.item()*nTotal)\n",
    "                    nTotals += nTotal\n",
    "            else:\n",
    "                for step in range(max_target_len):\n",
    "                    decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "                    # non teacher forcing: next input is current decoder output\n",
    "                    # choose the index of word with highest possibility\n",
    "                    _, indx = torch.max(decoder_output, dim=1) \n",
    "                    decoder_input = torch.LongTensor([[indx[i] for i in range(batch_size)]])\n",
    "                    decoder_input.to(device)\n",
    "                    mask_loss, nTotal = loss_func(decoder_output, target_t[step], mask_t[step])\n",
    "                    loss += mask_loss\n",
    "                    print_loss.append(mask_loss.item()*nTotal)\n",
    "                    nTotals += nTotal\n",
    "                    if decoder_input.item() == EOS_TOKEN:\n",
    "                        break\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1.0)\n",
    "            _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), 1.0)\n",
    "\n",
    "            encoder_opt.step()  # update weights\n",
    "            decoder_opt.step()\n",
    "            \n",
    "            iter_loss += sum(print_loss) / nTotals\n",
    "\n",
    "        print(\"Epoch: {}; Average Loss: {:.4}\".format(epoch + 1, iter_loss / n_iters))\n",
    "        \n",
    "        # save model\n",
    "        if epoch % 100 == 0:\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'enc': encoder.state_dict(),\n",
    "                'dec': decoder.state_dict(),\n",
    "                'enc_opt': encoder_opt.state_dict(),\n",
    "                'dec_opt': decoder_opt.state_dict(),\n",
    "                'loss': iter_loss / n_iters,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(folder, \"epoch{}.tar\".format(epoch)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part IV. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference shares the same encoder/decoder structure as training process\n",
    "class InferenceDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(InferenceDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, input_t, lengths, max_target_len):\n",
    "        encoder_output, encoder_hidden = encoder(input_t, lengths)\n",
    "        decoder_input = torch.LongTensor([[SOS_TOKEN]])\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layer]\n",
    "        preds = torch.LongTensor()\n",
    "        probs = torch.LongTensor()\n",
    "        \n",
    "        decoder_input = decoder_input.to(device)\n",
    "        preds = preds.to(device)\n",
    "        probs = probs.to(device)\n",
    "        \n",
    "        for _ in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_output)\n",
    "            prob, indx = torch.max(decoder_output, dim=1)\n",
    "            # non teacher forcing\n",
    "            decoder_input = torch.LongTensor([indx])\n",
    "            preds = torch.cat((preds, indx), dim=0)\n",
    "            probs = torch.cat((probs, prob), dim=0)\n",
    "        \n",
    "        return preds, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one test_seq at a time\n",
    "def evaluation(inference, voc, test_seq):    \n",
    "    # word2index\n",
    "    mapped_batch = [map_character(voc, test_seq)]\n",
    "    lengths = torch.tensor([len(mapped_batch[0])])\n",
    "    input_batch = torch.LongTensor(mapped_batch).t()\n",
    "    input_batch = input_batch.to(device)\n",
    "    \n",
    "    # max_target_length = 500\n",
    "    preds, probs = inference(input_batch, lengths, 500)\n",
    "    decoded_words = [voc.index2word[indx.item()] for indx in preds]\n",
    "    return decoded_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part V. Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "hidden_dim = 500\n",
    "enc_layer_dim = 2\n",
    "dec_layer_dim = 2\n",
    "dropout = 0.1\n",
    "attn_method = \"dot\"\n",
    "lr = 0.0001\n",
    "teacher_forcing_r = 0.5\n",
    "batch_size = 10\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape:  torch.Size([441, 10, 500])\n",
      "Encoder hidden shape:  torch.Size([4, 10, 500])\n",
      "Init decoder input:  torch.Size([1, 10])\n",
      "decoder hidden shape:  torch.Size([2, 10, 500])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|â–‰         | 7/71 [03:16<30:01, 28.16s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-c57550d35b41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m train(input_seq, target_seq, batch_size, epochs, hidden_dim, embedding, encoder, decoder,\n\u001b[0;32m---> 45\u001b[0;31m       encoder_opt, decoder_opt, teacher_forcing_r, epoch_from, folder)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-24-3fd5d7f66c65>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(input_seq, target_seq, batch_size, epochs, hidden_dim, embedding, encoder, decoder, encoder_opt, decoder_opt, teacher_forcing_r, epoch_from, folder)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mmask_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmask_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mSOS_TOKEN\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-b7d85f850d3e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_seq, input_length)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;31m# hidden: (n_layers * num_directions, batch_size, hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# unpack outputs: (max_length, batch_size, num_direction * hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m             result = _impl(input, batch_sizes, hx, self._flat_weights, self.bias,\n\u001b[0;32m--> 182\u001b[0;31m                            self.num_layers, self.dropout, self.training, self.bidirectional)\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LSTM'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## training process\n",
    "loadMode = None\n",
    "folder = os.path.join(\"checkpoints\", \"{}_{}_{}\".format(enc_layer_dim, dec_layer_dim, hidden_dim))\n",
    "\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)\n",
    "\n",
    "if loadMode:\n",
    "    checkpoint = torch.load(loadModel)\n",
    "    enc_chp = checkpoint[\"enc\"]\n",
    "    dec_chp = checkpoint[\"dec\"]\n",
    "    enc_opt_chp = checkpoint[\"enc_opt\"]\n",
    "    dec_opt_chp = checkpoint[\"dec_opt\"]\n",
    "    voc.__dict__ = checkpoint[\"voc_dict\"]\n",
    "    embedding_chp = checkpoint[\"embedding\"]\n",
    "    \n",
    "embedding = nn.Embedding(voc.num_words, hidden_dim)\n",
    "\n",
    "if loadMode:\n",
    "    embedding.load_state_dict(embedding_chp)\n",
    "\n",
    "encoder = Encoder(hidden_dim, embedding, enc_layer_dim, dropout)\n",
    "decoder = Decoder(attn_method, embedding, hidden_dim, voc.num_words, dec_layer_dim, dropout)\n",
    "\n",
    "if loadMode:\n",
    "    encoder.load_state_dict(enc_chp)\n",
    "    decoder.load_state_dict(dec_chp)\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "encoder_opt = optim.Adam(encoder.parameters(), lr=lr)\n",
    "decoder_opt = optim.Adam(decoder.parameters(), lr=lr)\n",
    "\n",
    "epoch_from = 0\n",
    "if loadMode:\n",
    "    encoder_opt.load_state_dict(enc_opt_chp)\n",
    "    decoder_opt = load_state_dict(dec_opt_chp)\n",
    "    epoch_from = checkpoint['epoch'] + 1\n",
    "        \n",
    "train(input_seq, target_seq, batch_size, epochs, hidden_dim, embedding, encoder, decoder,\n",
    "      encoder_opt, decoder_opt, teacher_forcing_r, epoch_from, folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
